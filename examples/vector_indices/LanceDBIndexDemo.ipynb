{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db0855d0",
   "metadata": {},
   "source": [
    "# LanceDB Index Demo\n",
    "In this notebook we are going to show how to use [LanceDB](https://www.lancedb.com) to perform vector searches in LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2d1c538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# Uncomment to see debug logs\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "from gpt_index import SimpleDirectoryReader, Document\n",
    "from gpt_index.indices.vector_store.vector_indices import GPTLanceDBIndex\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c71b6d",
   "metadata": {},
   "source": [
    "### Setup OpenAI\n",
    "The first step is to configure the openai key. It will be used to created embeddings for the documents loaded into the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67b86621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7010b1d-d1bb-4f08-9309-a328bb4ea396",
   "metadata": {},
   "source": [
    "### Loading documents\n",
    "Load the documents stored in the `paul_graham_essay/data` using the SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c154dd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: df5cfb31-d014-4780-9bc4-34a541544e35 Document Hash: 77ae91ab542f3abb308c4d7c77c9bc4c9ad0ccd63144802b7cbe7e1bb3a4094e\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\n",
    "print('Document ID:', documents[0].doc_id, 'Document Hash:', documents[0].doc_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0232fd1",
   "metadata": {},
   "source": [
    "### Create the index\n",
    "Here we create an index backed by LanceDB using the documents loaded previously. GPTLanceDBIndex takes a few arguments.\n",
    "- uri (str, required): Location where LanceDB will store its files.\n",
    "- table_name (str, optional): The table name where the embeddings will be stored. Defaults to \"vectors\".\n",
    "- nprobes (int, optional): The number of probes used. A higher number makes search more accurate but also slower. Defaults to 20.\n",
    "- refine_factor: (int, optional): Refine the results by reading extra elements and re-ranking them in memory. Defaults to None\n",
    "\n",
    "- More details can be found at the [LanceDB docs](https://lancedb.github.io/lancedb/ann_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8731da62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gpt_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 17617 tokens\n"
     ]
    }
   ],
   "source": [
    "index = GPTLanceDBIndex.from_documents(documents, uri=\"/tmp/lancedb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee4473a-094f-4d0a-a825-e1213db07240",
   "metadata": {},
   "source": [
    "### Query the index\n",
    "We can now ask questions using our index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a2bcc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gpt_index.token_counter.token_counter:> [query] Total LLM token usage: 3720 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total embedding token usage: 5 tokens\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"Who is the author?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cf55bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  The author of the text is Paul Graham, co-founder of Y Combinator.\n"
     ]
    }
   ],
   "source": [
    "print(textwrap.fill(str(response), 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68cbd239-880e-41a3-98d8-dbb3fab55431",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gpt_index.token_counter.token_counter:> [query] Total LLM token usage: 4082 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total embedding token usage: 8 tokens\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"What did the author do growing up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdf5287f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  The author grew up writing short stories, programming on an IBM 1401, and building a computer kit\n",
      "with a friend. They also wrote programs for a TRS-80 computer, such as games, a program to predict\n",
      "model rocket flight, and a word processor. In college, they studied philosophy and AI, and wrote a\n",
      "book about Lisp hacking. They also took art classes and applied to art schools, and while a student\n",
      "at the Accademia, they started painting still lives in their bedroom at night. These paintings were\n",
      "tiny, because the room was, and because they painted them on leftover scraps of canvas, which was\n",
      "all they could afford at the time. They also arrived at an arrangement with the faculty whereby the\n",
      "students wouldn't require the faculty to teach anything, and in return the faculty wouldn't require\n",
      "the students to learn anything. They even had a little stove, fed with kindling, that you see in\n",
      "19th century studio paintings, and a nude model sitting as close to it as possible without getting\n",
      "burned. The author also painted the model, while the other students spent their time chatting or\n",
      "occasionally trying to imitate things they'd seen in American art magazines. The model turned out to\n",
      "live just down the street from the author, and made a living from a\n"
     ]
    }
   ],
   "source": [
    "print(textwrap.fill(str(response), 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e603ff0",
   "metadata": {},
   "source": [
    "### Saving / Loading the Index\n",
    "You can save the index configuration for later usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba1558b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_index = index.save_to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172586fb",
   "metadata": {},
   "source": [
    "You can load the index from the saved information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ccec89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gpt_index.token_counter.token_counter:> [query] Total LLM token usage: 3720 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total embedding token usage: 5 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The author of the text is Paul Graham, co-founder of Y Combinator.\n"
     ]
    }
   ],
   "source": [
    "del index\n",
    "\n",
    "index = GPTLanceDBIndex.load_from_dict(saved_index)\n",
    "print(index.query(\"Who is the author?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afc84ac",
   "metadata": {},
   "source": [
    "### Appending data\n",
    "You can also add data to an existing index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "069fc099",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gpt_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 4 tokens\n"
     ]
    }
   ],
   "source": [
    "del index\n",
    "\n",
    "index = GPTLanceDBIndex.from_documents([Document(\"The sky is blue\")], uri=\"/tmp/new_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5cffcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gpt_index.token_counter.token_counter:> [query] Total LLM token usage: 43 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total embedding token usage: 5 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The author is unknown.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"Who is the author?\")\n",
    "print(textwrap.fill(str(response), 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc99404d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gpt_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 17617 tokens\n"
     ]
    }
   ],
   "source": [
    "index = GPTLanceDBIndex.from_documents(documents, uri=\"/tmp/new_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "676214a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gpt_index.token_counter.token_counter:> [query] Total LLM token usage: 3720 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total embedding token usage: 5 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  The author of the text is Paul Graham, co-founder of Y Combinator.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"Who is the author?\")\n",
    "print(textwrap.fill(str(response), 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
